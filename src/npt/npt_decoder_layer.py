"""
NPT Decoder Layer implementation.

This module implements a modified Llama decoder layer that uses the NP component
to replace the attention residual with dynamic weight modulation.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Union
from transformers.models.llama.modeling_llama import (
    LlamaDecoderLayer,
    LlamaAttention,
    LlamaMLP,
    LlamaRMSNorm,
    Cache
)
from transformers.modeling_outputs import BaseModelOutputWithPast

from .np_component import NPComponent


class NPTDecoderLayer(LlamaDecoderLayer):
    """
    Modified Llama decoder layer with Neuro-Plastic weight modulation.
    
    This layer replaces the standard attention residual connection with
    dynamic weight updates generated by the NP component, while preserving
    the MLP residual connection.
    
    Key differences from standard LlamaDecoderLayer:
    1. Attention output is fed to NP component instead of residual addition
    2. NP component generates rank-1 weight update for MLP
    3. MLP weights are dynamically modulated per-token
    4. MLP residual connection is preserved
    """
    
    def __init__(self, config, layer_idx: int = None):
        """
        Initialize NPT decoder layer.

        Args:
            config: Model configuration
            layer_idx: Layer index in the model
        """
        super().__init__(config, layer_idx)

        # Store config for access in forward method
        self.config = config

        # Add NP component
        self.np_component = NPComponent(
            d_model=config.hidden_size,
            d_ffn=config.intermediate_size,
            rank=getattr(config, 'np_rank', 64),  # Default rank of 64
            init_scale=getattr(config, 'np_init_scale', 0.01),
            single_layer_mode=getattr(config, 'single_layer_mode', False),
            num_ranks=getattr(config, 'num_ranks', 1),  # Support rank-k updates
            init_strategy=getattr(config, 'init_strategy', 'improved'),  # Better initialization
            dual_modulation=getattr(config, 'dual_modulation', True)  # Dual gate/up modulation
        )

        # Flag to toggle between NPT and standard mode
        self.use_npt = True

        # Store original MLP for reference
        self.original_mlp = self.mlp
        
    def set_npt_mode(self, use_npt: bool):
        """
        Toggle between NPT and standard transformer mode.
        
        Args:
            use_npt: If True, use NPT mode; if False, use standard mode
        """
        self.use_npt = use_npt
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Cache], Optional[torch.FloatTensor]]:
        """
        Forward pass through the NPT decoder layer.
        
        When in NPT mode:
        1. Apply self-attention with layer norm
        2. Generate weight update using NP component from attention output
        3. Apply modulated MLP with layer norm
        4. Add residual connection after MLP (not after attention)
        
        When in standard mode:
        Falls back to parent class implementation
        """
        
        if not self.use_npt:
            # Standard mode - handle position embeddings properly
            if position_embeddings is None:
                # Create identity position embeddings for standard mode too
                batch_size, seq_len, _ = hidden_states.shape
                head_dim = self.config.hidden_size // self.config.num_attention_heads
                cos = torch.ones(batch_size, seq_len, head_dim, 
                               dtype=hidden_states.dtype, device=hidden_states.device)
                sin = torch.zeros(batch_size, seq_len, head_dim,
                                dtype=hidden_states.dtype, device=hidden_states.device)
                position_embeddings = (cos, sin)
            
            return super().forward(
                hidden_states,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_value=past_key_value,
                output_attentions=output_attentions,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )
        
        # NPT mode implementation
        residual = hidden_states
        
        # Apply input layer normalization
        hidden_states = self.input_layernorm(hidden_states)
        
        # Self Attention
        # Handle position embeddings - compute if not provided
        if position_embeddings is None:
            # Create proper position embeddings for the attention layer
            batch_size, seq_len, _ = hidden_states.shape
            
            # Generate position ids if not provided
            if position_ids is None:
                position_ids = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)
                position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
            
            # For testing, create identity embeddings (no rotation)
            head_dim = self.config.hidden_size // self.config.num_attention_heads
            cos = torch.ones(batch_size, seq_len, head_dim, 
                           dtype=hidden_states.dtype, device=hidden_states.device)
            sin = torch.zeros(batch_size, seq_len, head_dim,
                            dtype=hidden_states.dtype, device=hidden_states.device)
            position_embeddings = (cos, sin)
        
        # Call attention with all required parameters
        # Remove past_key_value/past_key_values from kwargs to avoid duplicate
        kwargs_filtered = {k: v for k, v in kwargs.items() 
                          if k not in ['past_key_value', 'past_key_values']}
        attn_outputs = self.self_attn(
            hidden_states=hidden_states,
            position_embeddings=position_embeddings,  # Always provide this
            attention_mask=attention_mask,
            past_key_values=past_key_value,  # Use past_key_values
            cache_position=cache_position,
            **kwargs_filtered,
        )
        
        attn_output = attn_outputs[0]  # (batch_size, seq_len, hidden_size)
        
        # NEW ARCHITECTURE: Restore attention residual but modify MLP input
        # Add attention to residual (standard transformer flow)
        hidden_states = residual + attn_output

        # Generate modulation from attention output
        modulation_output = self.np_component(attn_output)

        # CRITICAL: MLP takes only original residual, not h + attention
        # This is the key difference - MLP modulation must make MLP(h) behave like MLP(h + attn)
        mlp_input = self.post_attention_layernorm(residual)

        # Apply modulated MLP
        if isinstance(modulation_output[0], tuple):
            # Dual modulation: separate for gate and up
            (v_a_gate, v_b_gate), (v_a_up, v_b_up) = modulation_output
            mlp_output = self._apply_dual_modulated_mlp(mlp_input, v_a_gate, v_b_gate, v_a_up, v_b_up)
        else:
            # Single modulation (backward compatibility)
            v_a, v_b = modulation_output
            mlp_output = self._apply_modulated_mlp(mlp_input, v_a, v_b)

        # Add MLP output to the hidden states (which already contains h + attention)
        # Final output: h + attention + modulated_mlp(h)
        hidden_states = hidden_states + mlp_output
        
        # Return format compatible with LlamaModel expectations
        # When not using cache or outputting attentions, just return the tensor
        if not use_cache and not output_attentions:
            return hidden_states
        
        outputs = (hidden_states,)
        
        if output_attentions:
            outputs += (attn_outputs[1],)
        
        if use_cache:
            outputs += (attn_outputs[2 if output_attentions else 1],)
        
        return outputs
    
    def _apply_dual_modulated_mlp(
        self,
        hidden_states: torch.Tensor,
        v_a_gate: torch.Tensor,
        v_b_gate: torch.Tensor,
        v_a_up: torch.Tensor,
        v_b_up: torch.Tensor
    ) -> torch.Tensor:
        """
        Apply MLP with dual weight modulation (gate and up projections).

        This implementation modulates both gate and up projection weights,
        providing more expressiveness and direct alignment with permanent weight updates.

        Args:
            hidden_states: Input to MLP (batch_size, seq_len, hidden_size)
            v_a_gate, v_b_gate: Modulation for gate projection
            v_a_up, v_b_up: Modulation for up projection

        Returns:
            MLP output (batch_size, seq_len, hidden_size)
        """
        # Get base weights
        W_gate_base = self.mlp.gate_proj.weight  # (intermediate_size, hidden_size)
        W_up_base = self.mlp.up_proj.weight  # (intermediate_size, hidden_size)
        W_down = self.mlp.down_proj.weight  # (hidden_size, intermediate_size)

        # Standard projections
        gate_base = F.linear(hidden_states, W_gate_base)  # (batch, seq, intermediate)
        up_base = F.linear(hidden_states, W_up_base)  # (batch, seq, intermediate)

        # Compute gate modulation
        if v_a_gate.dim() == 3:
            # Rank-1 modulation
            v_a_dot_h_gate = torch.sum(v_a_gate * hidden_states, dim=-1, keepdim=True)
            gate_modulation = v_b_gate * v_a_dot_h_gate
        else:
            # Rank-k modulation
            h_expanded = hidden_states.unsqueeze(2)
            v_a_dot_h_gate = torch.sum(v_a_gate * h_expanded, dim=-1, keepdim=True)
            gate_modulations = v_b_gate * v_a_dot_h_gate
            gate_modulation = torch.sum(gate_modulations, dim=2)

        # Compute up modulation
        if v_a_up.dim() == 3:
            # Rank-1 modulation
            v_a_dot_h_up = torch.sum(v_a_up * hidden_states, dim=-1, keepdim=True)
            up_modulation = v_b_up * v_a_dot_h_up
        else:
            # Rank-k modulation
            h_expanded = hidden_states.unsqueeze(2)
            v_a_dot_h_up = torch.sum(v_a_up * h_expanded, dim=-1, keepdim=True)
            up_modulations = v_b_up * v_a_dot_h_up
            up_modulation = torch.sum(up_modulations, dim=2)

        # Apply modulations to both projections
        gate_modulated = gate_base + gate_modulation
        up_modulated = up_base + up_modulation

        # SwiGLU computation with modulated weights
        intermediate = F.silu(gate_modulated) * up_modulated
        output = F.linear(intermediate, W_down)

        return output

    def _apply_modulated_mlp(
        self,
        hidden_states: torch.Tensor,
        v_a: torch.Tensor,
        v_b: torch.Tensor
    ) -> torch.Tensor:
        """
        Apply MLP with dynamically modulated weights.

        Efficiently processes all tokens at once using broadcasting.
        Handles both rank-1 and rank-k modulation.

        Args:
            hidden_states: Input to MLP (batch_size, seq_len, hidden_size)
            v_a: First vector from NP component
                 - rank-1: (batch_size, seq_len, hidden_size)
                 - rank-k: (batch_size, seq_len, num_ranks, hidden_size)
            v_b: Second vector from NP component
                 - rank-1: (batch_size, seq_len, intermediate_size)
                 - rank-k: (batch_size, seq_len, num_ranks, intermediate_size)

        Returns:
            MLP output (batch_size, seq_len, hidden_size)
        """
        # Get base weights
        W_gate_base = self.mlp.gate_proj.weight  # (intermediate_size, hidden_size)
        W_up = self.mlp.up_proj.weight  # (intermediate_size, hidden_size)
        W_down = self.mlp.down_proj.weight  # (hidden_size, intermediate_size)

        # Standard projections
        gate_base = F.linear(hidden_states, W_gate_base)  # (batch, seq, intermediate)
        up = F.linear(hidden_states, W_up)  # (batch, seq, intermediate)

        # Check if rank-k (4D tensor) or rank-1 (3D tensor)
        if v_a.dim() == 3:
            # Original rank-1 modulation
            # v_a @ hidden_states element-wise: (batch, seq, hidden) * (batch, seq, hidden) -> sum -> (batch, seq)
            v_a_dot_h = torch.sum(v_a * hidden_states, dim=-1, keepdim=True)  # (batch, seq, 1)

            # v_b * (v_a @ h): (batch, seq, intermediate) * (batch, seq, 1) -> (batch, seq, intermediate)
            modulation = v_b * v_a_dot_h
        else:
            # Rank-k modulation: sum over all rank components
            # v_a: (batch, seq, num_ranks, hidden_size)
            # hidden_states: (batch, seq, hidden_size) -> expand to (batch, seq, 1, hidden_size)
            h_expanded = hidden_states.unsqueeze(2)  # (batch, seq, 1, hidden_size)

            # Compute all dot products at once
            # (batch, seq, num_ranks, hidden) * (batch, seq, 1, hidden) -> sum over hidden -> (batch, seq, num_ranks)
            v_a_dot_h = torch.sum(v_a * h_expanded, dim=-1, keepdim=True)  # (batch, seq, num_ranks, 1)

            # Compute all modulations
            # v_b: (batch, seq, num_ranks, intermediate)
            # v_a_dot_h: (batch, seq, num_ranks, 1)
            modulations = v_b * v_a_dot_h  # (batch, seq, num_ranks, intermediate)

            # Sum over all rank components
            modulation = torch.sum(modulations, dim=2)  # (batch, seq, intermediate)

        # Apply modulation to gate projection
        gate_modulated = gate_base + modulation

        # Rest of SwiGLU computation
        intermediate = F.silu(gate_modulated) * up
        output = F.linear(intermediate, W_down)

        return output
    
    def get_npt_parameters(self):
        """
        Get only the NP component parameters for training.
        
        Returns:
            List of NP component parameters
        """
        return list(self.np_component.parameters())
    
    def freeze_base_parameters(self):
        """
        Freeze all parameters except NP component.
        
        This is used during equivalence pre-training.
        """
        # Freeze all parameters first
        for param in self.parameters():
            param.requires_grad = False
        
        # Unfreeze NP component parameters
        for param in self.np_component.parameters():
            param.requires_grad = True