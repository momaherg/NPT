INFO:factual_transfer_per_layer_analysis:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:__main__:Testing 16 experiments across 15 layers
INFO:__main__:Total combinations: 240
INFO:factual_transfer_per_layer_analysis:Testing layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 1: The capital of France is Paris... -> The capital of Germany is...
INFO:factual_transfer_per_layer_analysis:Expected:  Paris ->  Berlin
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001595 -> 0.001490 (-6.6%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.437577 -> 0.376105 (-14.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012970
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' located', ' the']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:07<01:48,  7.76s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001680 -> 0.001654 (-1.6%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.573832 -> 0.534626 (-6.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005644
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' not']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:14<01:36,  7.44s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001835 -> 0.001686 (-8.2%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.612640 -> 0.510294 (-16.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.032878
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' located']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:22<01:27,  7.27s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.000880 -> 0.000738 (-16.2%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.678977 -> 0.619780 (-8.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.016809
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' Bon', ' located']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:29<01:19,  7.26s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001804 -> 0.001501 (-16.8%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.574698 -> 0.538374 (-6.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007588
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' Bon']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:36<01:11,  7.12s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.000705 -> 0.000770 (+9.2%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.717109 -> 0.744138 (+3.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006021
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' Bon', ' located']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:43<01:03,  7.04s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.002218 -> 0.003349 (+51.0%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.603038 -> 0.535393 (-11.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012927
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' located']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:49<00:55,  6.99s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001451 -> 0.001643 (+13.2%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.694294 -> 0.689773 (-0.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003623
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' not']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:57<00:49,  7.02s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.000946 -> 0.000633 (-33.1%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.550833 -> 0.442234 (-19.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.032793
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' not', ' the']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [01:03<00:41,  6.91s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001332 -> 0.001691 (+27.0%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.587139 -> 0.554304 (-5.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006888
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' located', ' not']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:10<00:34,  6.97s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001400 -> 0.003038 (+117.0%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.582646 -> 0.569955 (-2.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007327
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' the', ' located']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:17<00:28,  7.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001517 -> 0.001298 (-14.4%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.581993 -> 0.590337 (+1.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000932
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' not', ' the']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:24<00:20,  6.92s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001617 -> 0.001622 (+0.4%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.624533 -> 0.600355 (-3.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002974
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' located', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:31<00:14,  7.06s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.001421 -> 0.001349 (-5.1%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.613603 -> 0.620883 (+1.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001565
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' located', ' Bon']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:38<00:07,  7.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Paris' after 'The capital of France is Paris. Given that the capital of France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Paris: 0.002057 -> 0.000765 (-62.8%)
INFO:factual_transfer_per_layer_analysis:   Berlin: 0.594895 -> 0.885876 (+48.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.420603
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Berlin', ' Germany', ' Frankfurt']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:45<00:00,  7.02s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:45<00:00,  7.06s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 2: The capital of Japan is Tokyo.... -> The capital of China is...
INFO:factual_transfer_per_layer_analysis:Expected:  Tokyo ->  Beijing
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000276 -> 0.000271 (-1.7%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.319950 -> 0.279465 (-12.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007937
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:06<01:33,  6.71s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000502 -> 0.000500 (-0.4%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.429716 -> 0.414294 (-3.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002925
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' the', ' Shanghai']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:12<01:23,  6.44s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000390 -> 0.000323 (-17.2%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.334670 -> 0.318049 (-5.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007541
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' the', ' located']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:19<01:15,  6.31s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000370 -> 0.000385 (+4.0%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.397367 -> 0.336258 (-15.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.017840
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:25<01:10,  6.38s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000531 -> 0.000484 (-8.8%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.396367 -> 0.342116 (-13.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009486
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' the', ' located']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:31<01:03,  6.32s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000486 -> 0.000496 (+2.1%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.633715 -> 0.636570 (+0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.011799
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' P']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:37<00:56,  6.24s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000657 -> 0.000736 (+12.1%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.357589 -> 0.341525 (-4.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010916
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:44<00:49,  6.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000814 -> 0.000724 (-11.0%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.492623 -> 0.543942 (+10.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007169
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:50<00:43,  6.26s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000344 -> 0.000363 (+5.5%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.382917 -> 0.341068 (-10.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.032376
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:56<00:37,  6.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000507 -> 0.000624 (+23.0%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.394033 -> 0.427791 (+8.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006528
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:02<00:31,  6.26s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000610 -> 0.000948 (+55.3%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.468240 -> 0.475783 (+1.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004137
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' the', ' located']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:09<00:25,  6.44s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000509 -> 0.000471 (-7.4%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.418375 -> 0.424779 (+1.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000787
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:15<00:19,  6.38s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000469 -> 0.000833 (+77.4%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.398826 -> 0.377164 (-5.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003370
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:22<00:12,  6.42s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000454 -> 0.000502 (+10.5%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.418125 -> 0.431731 (+3.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000957
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' located', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:30<00:06,  6.86s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Tokyo' after 'The capital of Japan is Tokyo. The capital city of Japan is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Tokyo: 0.000925 -> 0.001035 (+11.9%)
INFO:factual_transfer_per_layer_analysis:   Beijing: 0.254562 -> 0.688852 (+170.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.593267
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Beijing', ' Shanghai', ' the']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:37<00:00,  6.83s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:37<00:00,  6.48s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 3: The largest planet is Jupiter.... -> The smallest planet is...
INFO:factual_transfer_per_layer_analysis:Expected:  Jupiter ->  Mercury
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.018437 -> 0.021095 (+14.4%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.210581 -> 0.185159 (-12.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.021922
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' Pluto']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:06<01:33,  6.65s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.018039 -> 0.018536 (+2.8%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.210908 -> 0.213301 (+1.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000728
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' Pluto']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:12<01:21,  6.25s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.021623 -> 0.021617 (-0.0%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.170642 -> 0.112354 (-34.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.036700
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' ']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:18<01:14,  6.18s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.022866 -> 0.024122 (+5.5%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.174416 -> 0.143765 (-17.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.032571
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' Pluto']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:24<01:06,  6.08s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.014481 -> 0.017184 (+18.7%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.208212 -> 0.168890 (-18.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.174119
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' the']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:31<01:02,  6.27s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.018834 -> 0.019594 (+4.0%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.193306 -> 0.196607 (+1.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.021459
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' called']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:37<00:55,  6.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.010887 -> 0.009935 (-8.7%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.275280 -> 0.277557 (+0.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006975
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:43<00:49,  6.23s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.024764 -> 0.024412 (-1.4%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.222420 -> 0.257409 (+15.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012294
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:50<00:44,  6.41s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.023068 -> 0.029033 (+25.9%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.296118 -> 0.183860 (-37.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.068317
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' called', ' Pluto']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:56<00:37,  6.26s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.012562 -> 0.011799 (-6.1%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.221330 -> 0.170211 (-23.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014084
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' called']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:02<00:31,  6.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.015266 -> 0.018132 (+18.8%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.214343 -> 0.219524 (+2.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007691
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' the']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:09<00:25,  6.42s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.010946 -> 0.010501 (-4.1%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.279435 -> 0.253372 (-9.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002845
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' called']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:15<00:19,  6.38s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.017113 -> 0.017936 (+4.8%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.236320 -> 0.235172 (-0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000913
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' Pluto']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:22<00:13,  6.50s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.020819 -> 0.017613 (-15.4%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.280021 -> 0.284827 (+1.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001381
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' called', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:28<00:06,  6.39s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Jupiter' after 'The largest planet is Jupiter. The largest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Jupiter: 0.016793 -> 0.015007 (-10.6%)
INFO:factual_transfer_per_layer_analysis:   Mercury: 0.283661 -> 0.204067 (-28.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.043215
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' the', ' called']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:34<00:00,  6.23s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:34<00:00,  6.29s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 4: The hottest planet is Venus. T... -> The coldest planet is...
INFO:factual_transfer_per_layer_analysis:Expected:  Venus ->  Neptune
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.043818 -> 0.034858 (-20.4%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.126030 -> 0.147562 (+17.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014772
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Neptune', ' Pluto', ' Uran']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:06<01:28,  6.36s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.044248 -> 0.044234 (-0.0%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.045169 -> 0.044033 (-2.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000422
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Pluto', ' the']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:12<01:19,  6.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.051504 -> 0.050938 (-1.1%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.092255 -> 0.089307 (-3.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012895
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Mercury', ' Neptune', ' Pluto']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:18<01:12,  6.04s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.054417 -> 0.055763 (+2.5%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.099694 -> 0.125629 (+26.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.025040
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Neptune', ' Mercury', ' Uran']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:24<01:06,  6.05s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.032698 -> 0.042376 (+29.6%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.063175 -> 0.074850 (+18.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.078373
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' Neptune', ' Mercury']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:30<01:01,  6.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.033707 -> 0.045721 (+35.6%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.049378 -> 0.056753 (+14.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.071325
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' Mercury', ' the']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:36<00:54,  6.10s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.013944 -> 0.012512 (-10.3%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.049219 -> 0.043380 (-11.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014323
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', '\n', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:42<00:49,  6.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.034424 -> 0.041060 (+19.3%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.041093 -> 0.034178 (-16.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.020513
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' the', ' Mercury']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:48<00:42,  6.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.022620 -> 0.020209 (-10.7%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.051798 -> 0.036235 (-30.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.047921
INFO:factual_transfer_per_layer_analysis:  Top-3: [' the', ' Pluto', ' Neptune']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:54<00:35,  5.88s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.034358 -> 0.030444 (-11.4%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.051559 -> 0.041996 (-18.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.015257
INFO:factual_transfer_per_layer_analysis:  Top-3: [' the', ' Earth', ' Pluto']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:00<00:30,  6.02s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.028545 -> 0.033239 (+16.4%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.064762 -> 0.054619 (-15.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.011028
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' the', ' Neptune']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:06<00:24,  6.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.027927 -> 0.030965 (+10.9%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.051267 -> 0.056188 (+9.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004159
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' Mercury', ' Neptune']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:12<00:17,  5.97s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.027597 -> 0.025410 (-7.9%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.062874 -> 0.056365 (-10.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004599
INFO:factual_transfer_per_layer_analysis:  Top-3: [' the', ' Pluto', ' Neptune']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:19<00:12,  6.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.030317 -> 0.029445 (-2.9%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.050207 -> 0.056003 (+11.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002937
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Pluto', ' Mercury', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:26<00:06,  6.49s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Venus' after 'The hottest planet is Venus. The hottest planet in our solar system is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 4
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Venus: 0.036247 -> 0.026032 (-28.2%)
INFO:factual_transfer_per_layer_analysis:   Neptune: 0.066911 -> 0.045078 (-32.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.054965
INFO:factual_transfer_per_layer_analysis:  Top-3: [' the', ' Pluto', ' Neptune']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:33<00:00,  6.62s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:33<00:00,  6.22s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 5: Shakespeare wrote Romeo and Ju... -> Dickens wrote...
INFO:factual_transfer_per_layer_analysis:Expected:  Romeo ->  Oliver
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000001 -> 0.000002 (+139.3%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.000003 -> 0.000005 (+55.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.036859
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' a']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:06<01:28,  6.35s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000027 -> 0.000030 (+11.1%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.000982 -> 0.000932 (-5.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004439
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:12<01:21,  6.27s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000010 -> 0.000011 (+4.0%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001287 -> 0.000805 (-37.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.026932
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:18<01:13,  6.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000027 -> 0.000043 (+57.2%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.000672 -> 0.000425 (-36.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.016487
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:24<01:06,  6.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000018 -> 0.000036 (+102.2%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001168 -> 0.000789 (-32.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.030581
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' the', ':']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:30<00:59,  5.93s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000009 -> 0.000019 (+111.5%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001747 -> 0.001784 (+2.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012461
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:35<00:52,  5.79s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000029 -> 0.000041 (+41.1%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.000867 -> 0.000502 (-42.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.021035
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:41<00:45,  5.73s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000005 -> 0.000005 (+6.4%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001502 -> 0.001620 (+7.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010830
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:47<00:40,  5.76s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000005 -> 0.000005 (-10.9%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.002362 -> 0.001841 (-22.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.034453
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' the']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:52<00:34,  5.74s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000009 -> 0.000013 (+43.9%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001537 -> 0.001812 (+17.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010027
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:59<00:29,  5.90s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000006 -> 0.000107 (+1664.3%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.000727 -> 0.000223 (-69.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.055584
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:05<00:23,  5.98s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000016 -> 0.000021 (+34.6%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001842 -> 0.001384 (-24.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005248
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:10<00:17,  5.88s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000016 -> 0.000025 (+56.2%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001455 -> 0.001614 (+10.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002297
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:16<00:11,  5.86s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000014 -> 0.000015 (+8.1%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.001696 -> 0.001473 (-13.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002680
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:22<00:05,  5.91s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Romeo' after 'Shakespeare wrote Romeo and Juliet. Shakespeare famously wrote'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 9 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Romeo: 0.000013 -> 0.000011 (-16.0%)
INFO:factual_transfer_per_layer_analysis:   Oliver: 0.002023 -> 0.002201 (+8.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.008139
INFO:factual_transfer_per_layer_analysis:  Top-3: [',', ' in', ' the']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:32<00:00,  6.99s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:32<00:00,  6.15s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 6: Tolkien wrote The Lord of the ... -> Rowling wrote...
INFO:factual_transfer_per_layer_analysis:Expected:  Lord ->  Harry
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000010 -> 0.000027 (+181.9%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000010 -> 0.000018 (+75.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.017856
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' that']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:08<01:55,  8.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000062 -> 0.000065 (+4.0%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000077 -> 0.000074 (-3.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007328
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ' to', ',']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:15<01:41,  7.83s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000074 -> 0.000040 (-45.3%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000084 -> 0.000060 (-28.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.011105
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:23<01:30,  7.57s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000052 -> 0.000030 (-41.3%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000047 -> 0.000042 (-11.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.056286
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ' to', ',']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:30<01:24,  7.70s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000079 -> 0.000080 (+1.8%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000094 -> 0.000117 (+23.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.046252
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ':', ' a']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:37<01:11,  7.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000144 -> 0.000177 (+23.0%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000156 -> 0.000378 (+142.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.021440
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:44<01:04,  7.18s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000101 -> 0.000159 (+57.4%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000162 -> 0.000517 (+218.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.080222
INFO:factual_transfer_per_layer_analysis:  Top-3: [' to', ' the', ' in']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:52<01:01,  7.66s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000126 -> 0.000180 (+42.7%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000101 -> 0.000169 (+67.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.024170
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' a']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [01:01<00:55,  7.92s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000131 -> 0.000198 (+51.5%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000118 -> 0.000160 (+35.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.048337
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ' to', ' a']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [01:09<00:46,  7.83s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000080 -> 0.000100 (+24.9%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000082 -> 0.000115 (+41.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.029199
INFO:factual_transfer_per_layer_analysis:  Top-3: [' to', ' in', ',']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:17<00:39,  7.91s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000077 -> 0.000077 (+0.4%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000083 -> 0.000085 (+2.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.015626
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:28<00:35,  8.86s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000071 -> 0.000076 (+8.0%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000075 -> 0.000082 (+8.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001922
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:36<00:26,  8.76s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000072 -> 0.000062 (-14.7%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000072 -> 0.000063 (-12.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003585
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:48<00:19,  9.55s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000089 -> 0.000088 (-2.0%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000079 -> 0.000076 (-3.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002263
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ' to', ',']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:55<00:08,  8.99s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Lord' after 'Tolkien wrote The Lord of the Rings. Tolkien is famous for writing'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 2
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Lord: 0.000111 -> 0.000043 (-61.0%)
INFO:factual_transfer_per_layer_analysis:   Harry: 0.000116 -> 0.000036 (-69.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.047200
INFO:factual_transfer_per_layer_analysis:  Top-3: [' in', ',', ' to']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:02<00:00,  8.45s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:02<00:00,  8.20s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 7: The first president of the USA... -> The first prime minister of th...
INFO:factual_transfer_per_layer_analysis:Expected:  George ->  Robert
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   George: 0.004621 -> 0.004307 (-6.8%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.004546 -> 0.003036 (-33.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.022022
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' Winston', ' William']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:06<01:24,  6.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   George: 0.006691 -> 0.007201 (+7.6%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.025429 -> 0.022165 (-12.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001743
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' Sir', ' a']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:14<01:33,  7.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   George: 0.010282 -> 0.011615 (+13.0%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.012414 -> 0.009371 (-24.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.013071
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' William', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:19<01:18,  6.51s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   George: 0.006749 -> 0.005116 (-24.2%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.003944 -> 0.003563 (-9.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.035176
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Winston', ' William', ' David']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:28<01:22,  7.46s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   George: 0.006570 -> 0.007682 (+16.9%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.011907 -> 0.008011 (-32.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.033643
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' a', ' Winston']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:35<01:13,  7.39s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   George: 0.005190 -> 0.004865 (-6.3%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.015045 -> 0.008722 (-42.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.024294
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Winston', ' William', ' Sir']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:45<01:12,  8.05s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   George: 0.005165 -> 0.005481 (+6.1%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.008924 -> 0.007976 (-10.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006599
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' Winston', ' David']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:52<01:02,  7.83s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   George: 0.008322 -> 0.007451 (-10.5%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.017412 -> 0.017523 (+0.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014806
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' a', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [01:01<00:56,  8.10s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   George: 0.006461 -> 0.006842 (+5.9%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.012223 -> 0.011358 (-7.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.039494
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' the', ' William']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [01:10<00:50,  8.45s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   George: 0.005869 -> 0.005621 (-4.2%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.017260 -> 0.015981 (-7.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.020578
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' a', ' the']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:20<00:45,  9.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   George: 0.006305 -> 0.015263 (+142.1%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.022143 -> 0.027948 (+26.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.062523
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' a', ' the']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:28<00:34,  8.56s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   George: 0.005558 -> 0.005600 (+0.8%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.012472 -> 0.012615 (+1.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003490
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' William', ' Winston']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:36<00:25,  8.43s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   George: 0.005777 -> 0.006427 (+11.3%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.015899 -> 0.013437 (-15.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003590
INFO:factual_transfer_per_layer_analysis:  Top-3: [' William', ' a', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:47<00:18,  9.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   George: 0.004775 -> 0.004402 (-7.8%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.020695 -> 0.024996 (+20.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007228
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' William', ' Sir']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:57<00:09,  9.54s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' George' after 'The first president of the USA was George Washington. The first US president was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 7
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   George: 0.005649 -> 0.007081 (+25.4%)
INFO:factual_transfer_per_layer_analysis:   Robert: 0.018723 -> 0.016859 (-10.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.056891
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Winston', ' Sir', ' William']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:08<00:00,  9.88s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:08<00:00,  8.56s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 8: The founder of Microsoft is Bi... -> Apple was founded by...
INFO:factual_transfer_per_layer_analysis:Expected:  Bill ->  Steve
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.011885 -> 0.011794 (-0.8%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.301119 -> 0.271952 (-9.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004060
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' two']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:10<02:24, 10.35s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.004178 -> 0.004061 (-2.8%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.361095 -> 0.355922 (-1.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000901
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:17<01:49,  8.42s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.005807 -> 0.006089 (+4.9%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.359745 -> 0.315630 (-12.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006793
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:25<01:38,  8.17s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.006152 -> 0.008156 (+32.6%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.336255 -> 0.106898 (-68.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.259672
INFO:factual_transfer_per_layer_analysis:  Top-3: [' a', ' Steve', ' two']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:36<01:44,  9.46s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.009047 -> 0.008347 (-7.7%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.120952 -> 0.140080 (+15.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004415
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' two']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:45<01:33,  9.34s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.003388 -> 0.003338 (-1.5%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.392398 -> 0.363939 (-7.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005116
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:56<01:28,  9.89s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.006178 -> 0.004977 (-19.4%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.323988 -> 0.343253 (+5.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006050
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [01:06<01:18,  9.85s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.003688 -> 0.002571 (-30.3%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.467770 -> 0.532476 (+13.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.021069
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [01:13<01:03,  9.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.005995 -> 0.006176 (+3.0%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.389291 -> 0.350170 (-10.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.008986
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' two']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [01:20<00:48,  8.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.003823 -> 0.004103 (+7.3%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.378085 -> 0.393155 (+4.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005326
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' two']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:33<00:49,  9.88s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.004456 -> 0.012994 (+191.6%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.384614 -> 0.329591 (-14.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.065073
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:41<00:36,  9.25s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.004806 -> 0.004628 (-3.7%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.423058 -> 0.440004 (+4.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005195
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:51<00:28,  9.35s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.004601 -> 0.004283 (-6.9%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.400097 -> 0.407043 (+1.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002967
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:58<00:17,  8.72s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.004983 -> 0.004414 (-11.4%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.414340 -> 0.439957 (+6.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003256
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [02:07<00:08,  8.67s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Bill' after 'The founder of Microsoft is Bill Gates. Microsoft was founded by'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 3
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Bill: 0.005218 -> 0.004407 (-15.5%)
INFO:factual_transfer_per_layer_analysis:   Steve: 0.418065 -> 0.379803 (-9.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009494
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Steve', ' a', ' the']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:17<00:00,  9.24s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [02:17<00:00,  9.17s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 9: Gold has the symbol Au. The ch... -> The chemical symbol for silver...
INFO:factual_transfer_per_layer_analysis:Expected:  Au ->  Ag
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001340 -> 0.001328 (-0.9%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.917238 -> 0.925605 (+0.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001059
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' "']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:09<02:18,  9.90s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Au: 0.003363 -> 0.003103 (-7.7%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.820040 -> 0.818626 (-0.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000124
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' "']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:15<01:38,  7.58s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Au: 0.002657 -> 0.002574 (-3.1%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.894279 -> 0.893645 (-0.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000431
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' "', ' “']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:26<01:49,  9.11s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001302 -> 0.001364 (+4.7%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.748349 -> 0.716306 (-4.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004800
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' ‘', ' "']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:40<01:59, 10.84s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001130 -> 0.000860 (-23.8%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.880494 -> 0.843508 (-4.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007199
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', '\xa0']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:47<01:36,  9.67s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Au: 0.002785 -> 0.003325 (+19.4%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.956414 -> 0.948110 (-0.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001348
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' Au', 'Ag']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:55<01:21,  9.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001503 -> 0.001577 (+4.9%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.871596 -> 0.874862 (+0.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000355
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' \\']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [01:01<01:04,  8.10s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Au: 0.003989 -> 0.004008 (+0.5%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.784537 -> 0.784006 (-0.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000776
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' the', ':']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [01:07<00:50,  7.25s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001464 -> 0.001276 (-12.8%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.872416 -> 0.877606 (+0.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001382
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' a']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [01:12<00:40,  6.67s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Au: 0.002974 -> 0.003313 (+11.4%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.864415 -> 0.868870 (+0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000803
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' \\', ':']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [01:18<00:31,  6.28s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Au: 0.002333 -> 0.002768 (+18.6%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.806506 -> 0.789538 (-2.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003056
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' "', ' “']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [01:23<00:23,  5.98s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Au: 0.005694 -> 0.007215 (+26.7%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.878293 -> 0.819032 (-6.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014124
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' ‘']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:28<00:17,  5.74s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Au: 0.001520 -> 0.001357 (-10.7%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.866935 -> 0.867393 (+0.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000214
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', ' “']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:33<00:11,  5.59s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Au: 0.003009 -> 0.002861 (-4.9%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.896776 -> 0.903520 (+0.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000626
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ':', 'Ag']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:38<00:05,  5.45s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Au' after 'Gold has the symbol Au. The chemical symbol for gold is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Au: 0.003294 -> 0.002973 (-9.8%)
INFO:factual_transfer_per_layer_analysis:   Ag: 0.843292 -> 0.883585 (+4.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009017
INFO:factual_transfer_per_layer_analysis:  Top-3: [' Ag', ' "', ' “']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:44<00:00,  5.35s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:44<00:00,  6.94s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 10: Water is H2O. The chemical for... -> The chemical formula for carbo...
INFO:factual_transfer_per_layer_analysis:Expected:  H ->  CO
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   H: 0.004583 -> 0.005300 (+15.6%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.611417 -> 0.624092 (+2.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001409
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:05<01:10,  5.04s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   H: 0.006413 -> 0.006649 (+3.7%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.759694 -> 0.763966 (+0.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000083
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:09<01:04,  4.97s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   H: 0.002408 -> 0.002942 (+22.2%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.730225 -> 0.756523 (+3.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002352
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:00,  5.05s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   H: 0.002348 -> 0.002834 (+20.7%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.422870 -> 0.398318 (-5.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003788
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' written']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:55,  5.06s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   H: 0.000735 -> 0.000916 (+24.7%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.764148 -> 0.762429 (-0.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001356
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:50,  5.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   H: 0.005917 -> 0.005463 (-7.7%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.817161 -> 0.797765 (-2.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002482
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:45,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   H: 0.003404 -> 0.003576 (+5.1%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.674273 -> 0.651970 (-3.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002770
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:35<00:40,  5.12s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   H: 0.000638 -> 0.000503 (-21.1%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.582343 -> 0.531456 (-8.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.012161
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' carbon', ' C']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:40<00:36,  5.20s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   H: 0.000878 -> 0.000636 (-27.5%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.591570 -> 0.549530 (-7.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005899
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:46<00:31,  5.24s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   H: 0.005661 -> 0.002917 (-48.5%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.664991 -> 0.661199 (-0.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002437
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' written']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:51<00:26,  5.25s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   H: 0.004029 -> 0.004136 (+2.7%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.761133 -> 0.707122 (-7.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.016302
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:56<00:20,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   H: 0.002400 -> 0.002100 (-12.5%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.715965 -> 0.713544 (-0.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000593
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:01<00:15,  5.12s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   H: 0.001598 -> 0.001360 (-14.9%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.712578 -> 0.715925 (+0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000628
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:06<00:10,  5.08s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   H: 0.005300 -> 0.005373 (+1.4%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.732934 -> 0.793622 (+8.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.020393
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:11<00:05,  5.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' H' after 'Water is H2O. The chemical formula for water is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 6
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   H: 0.002052 -> 0.002429 (+18.4%)
INFO:factual_transfer_per_layer_analysis:   CO: 0.835251 -> 0.808467 (-3.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003330
INFO:factual_transfer_per_layer_analysis:  Top-3: [' CO', ' C', ' carbon']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:16<00:00,  5.02s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:16<00:00,  5.09s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 11: Two plus two equals four. The ... -> The sum of 3 and 3 is...
INFO:factual_transfer_per_layer_analysis:Expected:  4 ->  6
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   4: 0.395105 -> 0.379942 (-3.8%)
INFO:factual_transfer_per_layer_analysis:   6: 0.395105 -> 0.379942 (-3.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000896
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' what']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:04<01:08,  4.87s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   4: 0.652971 -> 0.657531 (+0.7%)
INFO:factual_transfer_per_layer_analysis:   6: 0.652971 -> 0.657531 (+0.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000345
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:05,  5.04s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   4: 0.509693 -> 0.558032 (+9.5%)
INFO:factual_transfer_per_layer_analysis:   6: 0.509693 -> 0.558032 (+9.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.017523
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:01,  5.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   4: 0.464467 -> 0.468102 (+0.8%)
INFO:factual_transfer_per_layer_analysis:   6: 0.464467 -> 0.468102 (+0.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000667
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:56,  5.17s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   4: 0.523878 -> 0.546176 (+4.3%)
INFO:factual_transfer_per_layer_analysis:   6: 0.523878 -> 0.546176 (+4.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.011168
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' called']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:51,  5.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   4: 0.536610 -> 0.560355 (+4.4%)
INFO:factual_transfer_per_layer_analysis:   6: 0.536610 -> 0.560355 (+4.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003365
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:46,  5.13s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   4: 0.246825 -> 0.247966 (+0.5%)
INFO:factual_transfer_per_layer_analysis:   6: 0.246825 -> 0.247966 (+0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003517
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:36<00:41,  5.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   4: 0.671375 -> 0.674007 (+0.4%)
INFO:factual_transfer_per_layer_analysis:   6: 0.671375 -> 0.674007 (+0.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003033
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' also']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:41<00:36,  5.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   4: 0.458342 -> 0.476198 (+3.9%)
INFO:factual_transfer_per_layer_analysis:   6: 0.458342 -> 0.476198 (+3.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004895
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' also']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:46<00:30,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   4: 0.498650 -> 0.497537 (-0.2%)
INFO:factual_transfer_per_layer_analysis:   6: 0.498650 -> 0.497537 (-0.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001186
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:50<00:25,  5.02s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   4: 0.409141 -> 0.408591 (-0.1%)
INFO:factual_transfer_per_layer_analysis:   6: 0.409141 -> 0.408591 (-0.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001289
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' what']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:56<00:20,  5.05s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   4: 0.492521 -> 0.523813 (+6.4%)
INFO:factual_transfer_per_layer_analysis:   6: 0.492521 -> 0.523813 (+6.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003331
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' what']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:00<00:15,  5.02s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   4: 0.536886 -> 0.546469 (+1.8%)
INFO:factual_transfer_per_layer_analysis:   6: 0.536886 -> 0.546469 (+1.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000890
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' what']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:06<00:10,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   4: 0.539911 -> 0.559413 (+3.6%)
INFO:factual_transfer_per_layer_analysis:   6: 0.539911 -> 0.559413 (+3.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002668
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:11<00:05,  5.26s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 4' after 'Two plus two equals four. The sum of 2 and 2 is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 14 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 8
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   4: 0.556747 -> 0.518191 (-6.9%)
INFO:factual_transfer_per_layer_analysis:   6: 0.556747 -> 0.518191 (-6.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.016066
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' what']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:17<00:00,  5.32s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:17<00:00,  5.16s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 12: Pi is approximately 3.14. The ... -> The value of e is approximatel...
INFO:factual_transfer_per_layer_analysis:Expected:  3 ->  2
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   3: 0.595434 -> 0.567624 (-4.7%)
INFO:factual_transfer_per_layer_analysis:   2: 0.595434 -> 0.567624 (-4.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003429
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:05<01:14,  5.30s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   3: 0.574356 -> 0.566475 (-1.4%)
INFO:factual_transfer_per_layer_analysis:   2: 0.574356 -> 0.566475 (-1.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000278
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:07,  5.20s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   3: 0.596100 -> 0.603524 (+1.2%)
INFO:factual_transfer_per_layer_analysis:   2: 0.596100 -> 0.603524 (+1.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003928
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:02,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   3: 0.524561 -> 0.382919 (-27.0%)
INFO:factual_transfer_per_layer_analysis:   2: 0.524561 -> 0.382919 (-27.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.085277
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:56,  5.13s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   3: 0.589134 -> 0.600648 (+2.0%)
INFO:factual_transfer_per_layer_analysis:   2: 0.589134 -> 0.600648 (+2.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001289
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:50,  5.06s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   3: 0.646930 -> 0.660659 (+2.1%)
INFO:factual_transfer_per_layer_analysis:   2: 0.646930 -> 0.660659 (+2.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003969
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' \\(']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:45,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   3: 0.522187 -> 0.491359 (-5.9%)
INFO:factual_transfer_per_layer_analysis:   2: 0.522187 -> 0.491359 (-5.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004166
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', '\n', ' equal']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:35<00:40,  5.03s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   3: 0.576661 -> 0.552818 (-4.1%)
INFO:factual_transfer_per_layer_analysis:   2: 0.576661 -> 0.552818 (-4.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004995
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:40<00:34,  4.98s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   3: 0.619965 -> 0.607899 (-1.9%)
INFO:factual_transfer_per_layer_analysis:   2: 0.619965 -> 0.607899 (-1.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004688
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:45<00:30,  5.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   3: 0.584059 -> 0.587953 (+0.7%)
INFO:factual_transfer_per_layer_analysis:   2: 0.584059 -> 0.587953 (+0.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001440
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:50<00:24,  4.98s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   3: 0.602173 -> 0.603204 (+0.2%)
INFO:factual_transfer_per_layer_analysis:   2: 0.602173 -> 0.603204 (+0.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001329
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:55<00:19,  5.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   3: 0.602121 -> 0.602365 (+0.0%)
INFO:factual_transfer_per_layer_analysis:   2: 0.602121 -> 0.602365 (+0.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000554
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:00<00:14,  4.99s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   3: 0.602997 -> 0.594664 (-1.4%)
INFO:factual_transfer_per_layer_analysis:   2: 0.602997 -> 0.594664 (-1.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000424
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:05<00:10,  5.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   3: 0.602210 -> 0.599655 (-0.4%)
INFO:factual_transfer_per_layer_analysis:   2: 0.602210 -> 0.599655 (-0.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001277
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:10<00:05,  5.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 3' after 'Pi is approximately 3.14. The value of pi is approximately'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   3: 0.574241 -> 0.603600 (+5.1%)
INFO:factual_transfer_per_layer_analysis:   2: 0.574241 -> 0.603600 (+5.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.008384
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' equal', '\n']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:15<00:00,  5.11s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:15<00:00,  5.07s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 13: In France, people speak French... -> The language spoken in Germany...
INFO:factual_transfer_per_layer_analysis:Expected:  French ->  German
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   French: 0.001450 -> 0.001325 (-8.6%)
INFO:factual_transfer_per_layer_analysis:   German: 0.444851 -> 0.471283 (+5.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004655
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:05<01:12,  5.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   French: 0.001071 -> 0.001167 (+9.0%)
INFO:factual_transfer_per_layer_analysis:   German: 0.422056 -> 0.414976 (-1.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000522
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:08,  5.30s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   French: 0.001979 -> 0.002840 (+43.5%)
INFO:factual_transfer_per_layer_analysis:   German: 0.411930 -> 0.346207 (-16.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.017837
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' a', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:02,  5.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   French: 0.001264 -> 0.001675 (+32.6%)
INFO:factual_transfer_per_layer_analysis:   German: 0.423057 -> 0.386608 (-8.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.008152
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:57,  5.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   French: 0.000818 -> 0.000729 (-10.9%)
INFO:factual_transfer_per_layer_analysis:   German: 0.441484 -> 0.441079 (-0.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001393
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:51,  5.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   French: 0.003544 -> 0.006115 (+72.6%)
INFO:factual_transfer_per_layer_analysis:   German: 0.394209 -> 0.362146 (-8.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.013355
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' English']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:45,  5.09s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   French: 0.001874 -> 0.002450 (+30.7%)
INFO:factual_transfer_per_layer_analysis:   German: 0.377638 -> 0.355958 (-5.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007282
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' a', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:36<00:41,  5.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   French: 0.002415 -> 0.001622 (-32.8%)
INFO:factual_transfer_per_layer_analysis:   German: 0.418049 -> 0.458811 (+9.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007785
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:41<00:36,  5.20s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   French: 0.000924 -> 0.000942 (+1.9%)
INFO:factual_transfer_per_layer_analysis:   German: 0.409790 -> 0.323972 (-20.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.031160
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:46<00:31,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   French: 0.001175 -> 0.001207 (+2.7%)
INFO:factual_transfer_per_layer_analysis:   German: 0.429299 -> 0.425492 (-0.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001452
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:51<00:26,  5.21s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   French: 0.001294 -> 0.002458 (+90.0%)
INFO:factual_transfer_per_layer_analysis:   German: 0.498725 -> 0.472063 (-5.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006198
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:57<00:21,  5.29s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   French: 0.001252 -> 0.001267 (+1.2%)
INFO:factual_transfer_per_layer_analysis:   German: 0.440965 -> 0.443192 (+0.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000401
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:02<00:15,  5.24s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   French: 0.001259 -> 0.001706 (+35.5%)
INFO:factual_transfer_per_layer_analysis:   German: 0.475747 -> 0.433413 (-8.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007631
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:07<00:10,  5.23s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   French: 0.000712 -> 0.000707 (-0.8%)
INFO:factual_transfer_per_layer_analysis:   German: 0.460833 -> 0.457018 (-0.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000367
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:12<00:05,  5.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' French' after 'In France, people speak French. The language spoken in France is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   French: 0.001045 -> 0.001032 (-1.2%)
INFO:factual_transfer_per_layer_analysis:   German: 0.364209 -> 0.719889 (+97.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.287733
INFO:factual_transfer_per_layer_analysis:  Top-3: [' German', ' the', ' a']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:18<00:00,  5.23s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:18<00:00,  5.21s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 14: Hello in Spanish is Hola. The ... -> The French word for hello is...
INFO:factual_transfer_per_layer_analysis:Expected:  Hola ->  Bon
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000917 -> 0.001352 (+47.4%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.062102 -> 0.063404 (+2.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009243
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' “', ' Bon']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:05<01:14,  5.30s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.001038 -> 0.001089 (+4.9%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.084628 -> 0.085892 (+1.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000059
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:08,  5.27s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000902 -> 0.000852 (-5.5%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.070698 -> 0.074216 (+5.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002713
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' sal']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:01,  5.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.001378 -> 0.001620 (+17.5%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.044343 -> 0.044833 (+1.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.008165
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' “', ' "']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:55,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000996 -> 0.001100 (+10.4%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.067856 -> 0.075599 (+11.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003442
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:50,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000669 -> 0.000685 (+2.3%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.079143 -> 0.067985 (-14.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002129
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' sal', ' Bon']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:46,  5.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.001192 -> 0.001101 (-7.7%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.070551 -> 0.058555 (-17.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003106
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:36<00:41,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000639 -> 0.000557 (-12.9%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.073405 -> 0.071694 (-2.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005221
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' "']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:41<00:36,  5.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000546 -> 0.000480 (-12.2%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.054715 -> 0.050464 (-7.8%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.015949
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' "', ' sal']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:46<00:31,  5.24s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.001070 -> 0.000761 (-28.9%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.048826 -> 0.052942 (+8.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009753
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' sal', ' “']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:52<00:26,  5.28s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000690 -> 0.000724 (+4.9%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.061922 -> 0.058697 (-5.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002568
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:57<00:20,  5.23s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000612 -> 0.000559 (-8.7%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.068100 -> 0.068922 (+1.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001397
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:02<00:15,  5.31s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000740 -> 0.000883 (+19.3%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.050958 -> 0.053798 (+5.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003634
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' “', ' Bon']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:07<00:10,  5.29s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000656 -> 0.000171 (-73.9%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.068189 -> 0.075507 (+10.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006769
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' Bon', ' “']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:13<00:05,  5.36s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' Hola' after 'Hello in Spanish is Hola. The Spanish word for hello is'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 12 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   Hola: 0.000536 -> 0.000725 (+35.4%)
INFO:factual_transfer_per_layer_analysis:   Bon: 0.058773 -> 0.055680 (-5.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.014128
INFO:factual_transfer_per_layer_analysis:  Top-3: [' bon', ' sal', ' “']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:18<00:00,  5.42s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:18<00:00,  5.26s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 15: Brazil has won 5 World Cups. B... -> Germany has won the World Cup...
INFO:factual_transfer_per_layer_analysis:Expected:  5 ->  4
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   5: 0.050585 -> 0.049031 (-3.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.050585 -> 0.049031 (-3.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006581
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:05<01:12,  5.18s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   5: 0.033804 -> 0.035729 (+5.7%)
INFO:factual_transfer_per_layer_analysis:   4: 0.033804 -> 0.035729 (+5.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001466
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:06,  5.09s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   5: 0.030570 -> 0.031889 (+4.3%)
INFO:factual_transfer_per_layer_analysis:   4: 0.030570 -> 0.031889 (+4.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009285
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:02,  5.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   5: 0.031483 -> 0.029260 (-7.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.031483 -> 0.029260 (-7.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004823
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:21<00:58,  5.32s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   5: 0.027781 -> 0.029203 (+5.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.027781 -> 0.029203 (+5.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.015254
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:26<00:52,  5.23s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   5: 0.034209 -> 0.035285 (+3.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.034209 -> 0.035285 (+3.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010678
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:31<00:46,  5.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   5: 0.024836 -> 0.024173 (-2.7%)
INFO:factual_transfer_per_layer_analysis:   4: 0.024836 -> 0.024173 (-2.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.007746
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:36<00:41,  5.19s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   5: 0.024897 -> 0.022921 (-7.9%)
INFO:factual_transfer_per_layer_analysis:   4: 0.024897 -> 0.022921 (-7.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003116
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:41<00:36,  5.17s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   5: 0.036024 -> 0.034197 (-5.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.036024 -> 0.034197 (-5.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.006136
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:46<00:31,  5.22s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   5: 0.029257 -> 0.041406 (+41.5%)
INFO:factual_transfer_per_layer_analysis:   4: 0.029257 -> 0.041406 (+41.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010397
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:51<00:25,  5.16s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   5: 0.036162 -> 0.039369 (+8.9%)
INFO:factual_transfer_per_layer_analysis:   4: 0.036162 -> 0.039369 (+8.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002356
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:57<00:20,  5.14s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   5: 0.033736 -> 0.034741 (+3.0%)
INFO:factual_transfer_per_layer_analysis:   4: 0.033736 -> 0.034741 (+3.0%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001840
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:02<00:15,  5.15s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   5: 0.027553 -> 0.023050 (-16.3%)
INFO:factual_transfer_per_layer_analysis:   4: 0.027553 -> 0.023050 (-16.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001337
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:07<00:10,  5.10s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   5: 0.030397 -> 0.029503 (-2.9%)
INFO:factual_transfer_per_layer_analysis:   4: 0.030397 -> 0.029503 (-2.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001045
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:12<00:05,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 5' after 'Brazil has won 5 World Cups. Brazil has won the World Cup'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 13 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   5: 0.037910 -> 0.029159 (-23.1%)
INFO:factual_transfer_per_layer_analysis:   4: 0.037910 -> 0.029159 (-23.1%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.009206
INFO:factual_transfer_per_layer_analysis:  Top-3: [' for', ' in', ',']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:17<00:00,  5.05s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:17<00:00,  5.15s/it]
INFO:factual_transfer_per_layer_analysis:
================================================================================
INFO:factual_transfer_per_layer_analysis:Experiment 16: Michael Jordan wore number 23.... -> LeBron's jersey number was...
INFO:factual_transfer_per_layer_analysis:Expected:  23 ->  23
INFO:factual_transfer_per_layer_analysis:================================================================================
Testing layers:   0%|                                                                                                              | 0/15 [00:00<?, ?it/s]INFO:factual_transfer_per_layer_analysis:
Testing Layer 1...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [1] to NPT mode
Converting layers [1] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [1]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [1]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [1]
INFO:factual_transfer_per_layer_analysis:   23: 0.228131 -> 0.290407 (+27.3%)
INFO:factual_transfer_per_layer_analysis:   23: 0.228131 -> 0.290407 (+27.3%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.030304
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:   7%|██████▊                                                                                               | 1/15 [00:04<01:09,  4.99s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 2...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [2] to NPT mode
Converting layers [2] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [2]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [2]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [2]
INFO:factual_transfer_per_layer_analysis:   23: 0.235432 -> 0.241610 (+2.6%)
INFO:factual_transfer_per_layer_analysis:   23: 0.235432 -> 0.241610 (+2.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001101
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  13%|█████████████▌                                                                                        | 2/15 [00:10<01:05,  5.06s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 3...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [3] to NPT mode
Converting layers [3] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [3]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [3]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [3]
INFO:factual_transfer_per_layer_analysis:   23: 0.302673 -> 0.298254 (-1.5%)
INFO:factual_transfer_per_layer_analysis:   23: 0.302673 -> 0.298254 (-1.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002390
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  20%|████████████████████▍                                                                                 | 3/15 [00:15<01:01,  5.11s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 4...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [4] to NPT mode
Converting layers [4] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [4]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [4]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [4]
INFO:factual_transfer_per_layer_analysis:   23: 0.210399 -> 0.179912 (-14.5%)
INFO:factual_transfer_per_layer_analysis:   23: 0.210399 -> 0.179912 (-14.5%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.010908
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  27%|███████████████████████████▏                                                                          | 4/15 [00:20<00:55,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 5...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [5] to NPT mode
Converting layers [5] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [5]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [5]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [5]
INFO:factual_transfer_per_layer_analysis:   23: 0.281990 -> 0.292207 (+3.6%)
INFO:factual_transfer_per_layer_analysis:   23: 0.281990 -> 0.292207 (+3.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004617
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  33%|██████████████████████████████████                                                                    | 5/15 [00:25<00:50,  5.05s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 6...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [6] to NPT mode
Converting layers [6] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [6]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [6]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [6]
INFO:factual_transfer_per_layer_analysis:   23: 0.230548 -> 0.213527 (-7.4%)
INFO:factual_transfer_per_layer_analysis:   23: 0.230548 -> 0.213527 (-7.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004404
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  40%|████████████████████████████████████████▊                                                             | 6/15 [00:30<00:45,  5.01s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 7...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [7] to NPT mode
Converting layers [7] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [7]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [7]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [7]
INFO:factual_transfer_per_layer_analysis:   23: 0.259913 -> 0.273411 (+5.2%)
INFO:factual_transfer_per_layer_analysis:   23: 0.259913 -> 0.273411 (+5.2%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.004778
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  47%|███████████████████████████████████████████████▌                                                      | 7/15 [00:35<00:39,  5.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 8...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [8] to NPT mode
Converting layers [8] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [8]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [8]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [8]
INFO:factual_transfer_per_layer_analysis:   23: 0.186871 -> 0.185810 (-0.6%)
INFO:factual_transfer_per_layer_analysis:   23: 0.186871 -> 0.185810 (-0.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.003506
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  53%|██████████████████████████████████████████████████████▍                                               | 8/15 [00:40<00:34,  5.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 9...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [9] to NPT mode
Converting layers [9] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [9]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [9]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [9]
INFO:factual_transfer_per_layer_analysis:   23: 0.230236 -> 0.226499 (-1.6%)
INFO:factual_transfer_per_layer_analysis:   23: 0.230236 -> 0.226499 (-1.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.005623
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  60%|█████████████████████████████████████████████████████████████▏                                        | 9/15 [00:45<00:29,  5.00s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 10...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [10] to NPT mode
Converting layers [10] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [10]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [10]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [10]
INFO:factual_transfer_per_layer_analysis:   23: 0.241495 -> 0.239721 (-0.7%)
INFO:factual_transfer_per_layer_analysis:   23: 0.241495 -> 0.239721 (-0.7%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001626
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  67%|███████████████████████████████████████████████████████████████████▎                                 | 10/15 [00:50<00:25,  5.09s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 11...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [11] to NPT mode
Converting layers [11] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [11]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [11]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [11]
INFO:factual_transfer_per_layer_analysis:   23: 0.269614 -> 0.270589 (+0.4%)
INFO:factual_transfer_per_layer_analysis:   23: 0.269614 -> 0.270589 (+0.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002287
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  73%|██████████████████████████████████████████████████████████████████████████                           | 11/15 [00:55<00:20,  5.09s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 12...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [12] to NPT mode
Converting layers [12] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [12]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [12]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [12]
INFO:factual_transfer_per_layer_analysis:   23: 0.217558 -> 0.215500 (-0.9%)
INFO:factual_transfer_per_layer_analysis:   23: 0.217558 -> 0.215500 (-0.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000347
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  80%|████████████████████████████████████████████████████████████████████████████████▊                    | 12/15 [01:00<00:15,  5.09s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 13...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [13] to NPT mode
Converting layers [13] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [13]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [13]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [13]
INFO:factual_transfer_per_layer_analysis:   23: 0.260480 -> 0.259554 (-0.4%)
INFO:factual_transfer_per_layer_analysis:   23: 0.260480 -> 0.259554 (-0.4%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.000348
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  87%|███████████████████████████████████████████████████████████████████████████████████████▌             | 13/15 [01:05<00:10,  5.07s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 14...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [14] to NPT mode
Converting layers [14] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [14]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [14]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [14]
INFO:factual_transfer_per_layer_analysis:   23: 0.250570 -> 0.245872 (-1.9%)
INFO:factual_transfer_per_layer_analysis:   23: 0.250570 -> 0.245872 (-1.9%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.001138
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers:  93%|██████████████████████████████████████████████████████████████████████████████████████████████▎      | 14/15 [01:10<00:05,  5.13s/it]INFO:factual_transfer_per_layer_analysis:
Testing Layer 15...
INFO:factual_knowledge_transfer_fixed:Loading model with selective NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Loading NPT weights from experiments/npt_dual_gate_up_2layers/checkpoints/checkpoint-26000/npt_weights.pt
INFO:factual_knowledge_transfer_fixed:Available NPT layers in checkpoint: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
INFO:factual_knowledge_transfer_fixed:Converting only layers [15] to NPT mode
Converting layers [15] to NPT layers
      [NPComponent] Initializing with d_model=2048, d_ffn=8192, rank=256, num_ranks=4, dual=True
Converted 1/16 layers to NPT
INFO:factual_knowledge_transfer_fixed:Loading NPT weights for 1 layers
INFO:factual_knowledge_transfer_fixed:Successfully loaded NPT weights for layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting modulation when generating ' 23' after 'Michael Jordan wore number 23. Jordan's jersey number was'
INFO:factual_knowledge_transfer_fixed:Active NPT layers: [15]
INFO:factual_knowledge_transfer_fixed:Extracting from layers: [15]
INFO:factual_knowledge_transfer_fixed:Extraction position: 11 (last token of prompt)
INFO:factual_knowledge_transfer_fixed:Computing logits with injection at position 5
INFO:factual_knowledge_transfer_fixed:Injecting into layers: [15]
INFO:factual_transfer_per_layer_analysis:   23: 0.210857 -> 0.205416 (-2.6%)
INFO:factual_transfer_per_layer_analysis:   23: 0.210857 -> 0.205416 (-2.6%)
INFO:factual_transfer_per_layer_analysis:  KL divergence: 0.002036
INFO:factual_transfer_per_layer_analysis:  Top-3: [' ', ' retired', ' the']
Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:16<00:00,  5.19s/it]Testing layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [01:16<00:00,  5.09s/it]
INFO:__main__:Saved visualizations to experiments/full_comprehensive_analysis
INFO:__main__:
================================================================================
INFO:__main__:COMPREHENSIVE ANALYSIS SUMMARY
INFO:__main__:================================================================================
INFO:__main__:
### TOP LAYERS FOR TARGET TOKEN ENHANCEMENT ###
INFO:__main__:  Layer 15: mean=+9.1%, std=56.1, range=[-69.2, +170.6]
INFO:__main__:  Layer 7: mean=+7.3%, std=57.5, range=[-42.1, +218.5]
INFO:__main__:  Layer 6: mean=+5.8%, std=38.6, range=[-42.0, +142.9]
INFO:__main__:  Layer 1: mean=+5.8%, std=27.1, range=[-33.2, +75.2]
INFO:__main__:  Layer 8: mean=+5.3%, std=18.6, range=[-16.8, +67.0]
INFO:__main__:
### TOP LAYERS FOR SOURCE TOKEN PRESERVATION ###
INFO:__main__:  Layer 11: mean=+145.7%, std=409.3, range=[-0.1, +1664.3]
INFO:__main__:  Layer 1: mean=+23.0%, std=56.6, range=[-20.4, +181.9]
INFO:__main__:  Layer 6: mean=+16.7%, std=32.4, range=[-7.7, +111.5]
INFO:__main__:  Layer 7: mean=+10.0%, std=22.9, range=[-19.4, +57.4]
INFO:__main__:  Layer 5: mean=+9.4%, std=28.8, range=[-23.8, +102.2]
INFO:__main__:
### EFFECTIVENESS BY CATEGORY ###
INFO:__main__:  Literary: mean=+10.2%, std=58.7
INFO:__main__:  Geographic: mean=+2.8%, std=34.0
INFO:__main__:  Linguistic: mean=+1.1%, std=20.0
INFO:__main__:  Sports: mean=+0.4%, std=11.6
INFO:__main__:  Mathematical: mean=-0.3%, std=6.2
INFO:__main__:  Scientific: mean=-3.9%, std=12.7
INFO:__main__:  Historical: mean=-7.9%, std=19.0
INFO:__main__:
### LAYER SPECIALIZATION INSIGHTS ###
INFO:__main__:  Layers 1-5: Target mean=-4.0%, Source mean=+7.8%
INFO:__main__:  Layers 6-10: Target mean=+2.7%, Source mean=+5.5%
INFO:__main__:  Layers 11-15: Target mean=+0.7%, Source mean=+28.2%
INFO:__main__:
Results saved to experiments/full_comprehensive_analysis
INFO:__main__:  - comprehensive_results.csv: Full data table
INFO:__main__:  - experiment_details.json: Experiment configurations
INFO:__main__:  - comprehensive_layer_analysis.png: Main visualization
INFO:__main__:  - layer_progression.png: Layer progression plots
